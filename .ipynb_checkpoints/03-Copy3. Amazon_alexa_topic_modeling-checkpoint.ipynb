{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-24T06:20:59.282958Z",
     "start_time": "2018-08-24T06:20:59.273565Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/xzhou/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /Users/xzhou/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import itertools\n",
    "import csv\n",
    "import string\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from collections import Counter\n",
    "import re\n",
    "import spacy\n",
    "import nltk\n",
    "from spacy.lang.en import English\n",
    "from nltk.util import ngrams\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.decomposition import NMF, TruncatedSVD\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import silhouette_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-24T06:04:29.439797Z",
     "start_time": "2018-08-24T06:04:29.434111Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.style.use('seaborn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-24T06:05:13.148589Z",
     "start_time": "2018-08-24T06:05:13.003125Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rating</th>\n",
       "      <th>date</th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>2018-07-31</td>\n",
       "      <td>Sometimes while playing a game, you can answer...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>2018-07-31</td>\n",
       "      <td>I have had a lot of fun with this thing. My 4 ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>2018-07-31</td>\n",
       "      <td>I received the echo as a gift. I needed anothe...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3</td>\n",
       "      <td>2018-07-31</td>\n",
       "      <td>Without having a cellphone, I cannot use many ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>5</td>\n",
       "      <td>2018-07-31</td>\n",
       "      <td>I think this is the 5th one I've purchased. I'...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   rating       date                                             review  \\\n",
       "2       4 2018-07-31  Sometimes while playing a game, you can answer...   \n",
       "3       5 2018-07-31  I have had a lot of fun with this thing. My 4 ...   \n",
       "5       5 2018-07-31  I received the echo as a gift. I needed anothe...   \n",
       "6       3 2018-07-31  Without having a cellphone, I cannot use many ...   \n",
       "7       5 2018-07-31  I think this is the 5th one I've purchased. I'...   \n",
       "\n",
       "   sentiment  \n",
       "2          1  \n",
       "3          1  \n",
       "5          1  \n",
       "6          0  \n",
       "7          1  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pkl_path = '/Users/xzhou/github/project_files/amazon/amazon_alexa_reviews_w_sentiment.pkl'\n",
    "\n",
    "with open(pkl_path, 'rb') as pkl_file:\n",
    "    df = pd.read_pickle(pkl_file)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-24T06:10:44.697772Z",
     "start_time": "2018-08-24T06:10:44.683300Z"
    }
   },
   "outputs": [],
   "source": [
    "# Slice data based on sentiment\n",
    "\n",
    "df_positive = df[df['sentiment']==1]\n",
    "df_neutral = df[df['sentiment']==0]\n",
    "df_negative = df[df['sentiment']==-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-24T06:10:45.333643Z",
     "start_time": "2018-08-24T06:10:45.328875Z"
    }
   },
   "outputs": [],
   "source": [
    "X = df['review']\n",
    "X_positive = df_positive['review']\n",
    "X_neutral = df_neutral['review']\n",
    "X_negative = df_negative['review']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-24T06:21:26.912266Z",
     "start_time": "2018-08-24T06:21:26.908382Z"
    }
   },
   "outputs": [],
   "source": [
    "# Use WordNetLemmatizer to get the root word.\n",
    "\n",
    "def get_lemma(word):\n",
    "    try:\n",
    "        lemma = wn.morphy(word)\n",
    "    except:\n",
    "        lemma = word\n",
    "    return lemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-24T06:23:39.818255Z",
     "start_time": "2018-08-24T06:23:39.810915Z"
    }
   },
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('English'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-24T06:31:13.438040Z",
     "start_time": "2018-08-24T06:31:13.434482Z"
    }
   },
   "outputs": [],
   "source": [
    "irrelevant_words = set(['get', 'one', 'say', 'still', 'amazon',\n",
    "                        'try', 'need', '34', 'echo', 'dot', 'even'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-24T06:31:13.656008Z",
     "start_time": "2018-08-24T06:31:13.653042Z"
    }
   },
   "outputs": [],
   "source": [
    "stop_words = stop_words.union(irrelevant_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-24T06:31:13.867119Z",
     "start_time": "2018-08-24T06:31:13.861503Z"
    }
   },
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    lda_tokens = []\n",
    "    text=text.strip().lower()\n",
    "    \n",
    "    # Extract word portion out of sentence and excludes spaace\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    tokens = [token for token in tokens if (not token.isspace())]\n",
    "    \n",
    "    for token in tokens:\n",
    "        lda_tokens.append(token)\n",
    "        \n",
    "    # Extract bigrams for additional analysis\n",
    "    bigrams = ngrams(tokens, 2)\n",
    "     \n",
    "    for bg, count in Counter(bigrams).most_common():\n",
    "        token_string = []\n",
    "        token_string = str(bg[0]) + \" \" + str(bg[1])\n",
    "        lda_tokens.append(token_string)\n",
    "\n",
    "    return lda_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-24T06:31:14.845269Z",
     "start_time": "2018-08-24T06:31:14.840769Z"
    }
   },
   "outputs": [],
   "source": [
    "def text_processing(sentence):\n",
    "    \n",
    "    tokens = tokenize(sentence)\n",
    "    tokens = [word for word in tokens if len(tokens)>5]\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    tokens = [get_lemma(word) for word in tokens]\n",
    "    \n",
    "    return tokens    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-24T06:31:16.288712Z",
     "start_time": "2018-08-24T06:31:15.320508Z"
    }
   },
   "outputs": [],
   "source": [
    "text_data = []\n",
    "\n",
    "for line in X_negative:\n",
    "    tokens = text_processing(line)\n",
    "    tokens=[token for token in tokens if token is not None]\n",
    "    text_data.append(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-24T06:31:23.765282Z",
     "start_time": "2018-08-24T06:31:23.704849Z"
    }
   },
   "outputs": [],
   "source": [
    "dictionary = corpora.Dictionary(text_data)\n",
    "corpus = [dictionary.doc2bow(text) for text in text_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-24T06:31:39.872614Z",
     "start_time": "2018-08-24T06:31:29.810826Z"
    }
   },
   "outputs": [],
   "source": [
    "num_topics = 10\n",
    "\n",
    "lda = LdaModel(corpus=corpus,  num_topics=num_topics, id2word=dictionary, passes=15) \n",
    "lda.save('test')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-24T06:31:47.434348Z",
     "start_time": "2018-08-24T06:31:47.426867Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, '0.014*\"music\" + 0.014*\"back\" + 0.013*\"return\" + 0.013*\"buy\" + 0.011*\"time\" + 0.011*\"works\" + 0.010*\"sent\" + 0.010*\"work\" + 0.008*\"respond\" + 0.008*\"get\"')\n",
      "(1, '0.014*\"device\" + 0.013*\"product\" + 0.012*\"warranty\" + 0.011*\"like\" + 0.010*\"month\" + 0.008*\"support\" + 0.008*\"3\" + 0.008*\"light\" + 0.008*\"customer\" + 0.007*\"work\"')\n",
      "(2, '0.021*\"question\" + 0.017*\"music\" + 0.017*\"ask\" + 0.015*\"answer\" + 0.013*\"time\" + 0.013*\"use\" + 0.010*\"work\" + 0.010*\"money\" + 0.010*\"like\" + 0.010*\"know\"')\n",
      "(3, '0.015*\"song\" + 0.014*\"music\" + 0.013*\"play\" + 0.010*\"use\" + 0.009*\"speaker\" + 0.007*\"device\" + 0.007*\"thing\" + 0.007*\"understand\" + 0.007*\"never\" + 0.006*\"like\"')\n",
      "(4, '0.030*\"work\" + 0.015*\"device\" + 0.013*\"devices\" + 0.012*\"buy\" + 0.011*\"problem\" + 0.010*\"support\" + 0.009*\"another\" + 0.009*\"want\" + 0.008*\"time\" + 0.008*\"money\"')\n",
      "(5, '0.029*\"connect\" + 0.018*\"music\" + 0.016*\"work\" + 0.015*\"speaker\" + 0.015*\"play\" + 0.011*\"google\" + 0.011*\"internet\" + 0.011*\"terrible\" + 0.010*\"like\" + 0.010*\"use\"')\n",
      "(6, '0.019*\"work\" + 0.016*\"buy\" + 0.011*\"use\" + 0.011*\"device\" + 0.011*\"try\" + 0.010*\"tell\" + 0.009*\"month\" + 0.009*\"time\" + 0.008*\"working\" + 0.008*\"know\"')\n",
      "(7, '0.017*\"use\" + 0.017*\"product\" + 0.011*\"need\" + 0.010*\"speak\" + 0.008*\"words\" + 0.008*\"connect\" + 0.007*\"show\" + 0.007*\"disappoint\" + 0.007*\"saying\" + 0.007*\"try\"')\n",
      "(8, '0.015*\"music\" + 0.012*\"device\" + 0.010*\"like\" + 0.010*\"disappoint\" + 0.009*\"month\" + 0.009*\"play\" + 0.009*\"want\" + 0.009*\"product\" + 0.009*\"order\" + 0.008*\"purchase\"')\n",
      "(9, '0.032*\"sound\" + 0.020*\"speaker\" + 0.016*\"quality\" + 0.014*\"volume\" + 0.010*\"much\" + 0.010*\"loud\" + 0.009*\"use\" + 0.009*\"poor\" + 0.009*\"way\" + 0.009*\"really\"')\n"
     ]
    }
   ],
   "source": [
    "topics = lda.print_topics(num_words=10)\n",
    "for topic in topics:\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-24T07:16:19.802970Z",
     "start_time": "2018-08-24T07:16:19.790679Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define a topic modeling engine for repetitive use. \n",
    "# This can help test out models and identify optimal result.\n",
    "\n",
    "class TopicModelEngine:\n",
    "    \n",
    "    def __init__(self, raw_data):\n",
    "        self.raw_data = raw_data\n",
    "        self.stop_words = set(stopwords.words('English'))\n",
    "        \n",
    "        \n",
    "    def get_lemma(self, word):\n",
    "        \"\"\"\n",
    "        Get the root words\n",
    "        \"\"\"\n",
    "        try:\n",
    "            lemma = wn.morphy(word)\n",
    "        except:\n",
    "            lemma = word\n",
    "        return lemma\n",
    "    \n",
    "    def stop_words_extention(self, add_stop_words):\n",
    "        \"\"\"\n",
    "        This is to extend list of stop words. Need to provide a list.  \n",
    "        \"\"\"\n",
    "        add_words_set = set (add_stop_words)\n",
    "        self.stop_words = self.stop_words.union(add_words_set)\n",
    "        \n",
    "        \n",
    "    def tokenize(self, text):\n",
    "        \"\"\"\n",
    "        This is to perform word tokenizing. It performs ngram (1,2)\n",
    "        \"\"\"\n",
    "        lda_tokens = []\n",
    "        text=text.strip().lower()\n",
    "\n",
    "        # Extract word portion out of sentence and excludes spaace\n",
    "        tokenizer = RegexpTokenizer(r'\\w+')\n",
    "        tokens = tokenizer.tokenize(text)\n",
    "        tokens = [token for token in tokens if (not token.isspace())]\n",
    "\n",
    "        for token in tokens:\n",
    "            lda_tokens.append(token)\n",
    "\n",
    "        # Extract bigrams for additional analysis\n",
    "        bigrams = ngrams(tokens, 2)\n",
    "\n",
    "        for bg, count in Counter(bigrams).most_common():\n",
    "            token_string = []\n",
    "            token_string = str(bg[0]) + \" \" + str(bg[1])\n",
    "            lda_tokens.append(token_string)\n",
    "\n",
    "        return lda_tokens\n",
    "    \n",
    "    def text_processing(self, sentence):\n",
    "        \"\"\"\n",
    "        This is to break down text sentence into words, and \n",
    "        clean up the words to prepare for LDA modeling.\n",
    "        \"\"\"\n",
    "    \n",
    "        tokens = tokenize(sentence)\n",
    "        tokens = [word for word in tokens if len(tokens)>5]\n",
    "        tokens = [word for word in tokens if word not in stop_words]\n",
    "        tokens = [get_lemma(word) for word in tokens]\n",
    "\n",
    "        return tokens \n",
    "    \n",
    "    def LDA_preprocessing(self):\n",
    "        \"\"\"\n",
    "        Provided with raw data, the proprocessing will automatically prepare and\n",
    "        clean up the data, and generate necessay documents for LDA's use.\n",
    "        \"\"\"\n",
    "        self.text_data = []\n",
    "\n",
    "        for line in self.raw_data:\n",
    "            tokens = text_processing(line)\n",
    "            tokens=[token for token in tokens if token is not None]\n",
    "            self.text_data.append(tokens)\n",
    "        \n",
    "        self.dictionary = corpora.Dictionary(self.text_data)\n",
    "        self.corpus = [dictionary.doc2bow(text) for text in self.text_data]\n",
    "\n",
    "    def LDA_model(self, num_of_topics, file_name_to_save_as):\n",
    "        \"\"\"\n",
    "        This is to perform topic modeling and save the model under the name\n",
    "        user provides\n",
    "        \"\"\"\n",
    "        self.LDA_preprocessing()\n",
    "        num_topics = num_of_topics\n",
    "        self.lda = LdaModel(corpus=self.corpus,  num_topics=num_topics, \n",
    "                       id2word=self.dictionary, passes=15) \n",
    "        lda.save(file_name_to_save_as)\n",
    "        \n",
    "        return self.lda\n",
    "    \n",
    "    def print_topics(self, num_of_words):\n",
    "        \"\"\"\n",
    "        Print out topics for users's decision making. \n",
    "        User can specify number of words to print out\n",
    "        \"\"\"\n",
    "        topics = lda.print_topics(num_words=num_of_words)\n",
    "        for topic in topics:\n",
    "            print('Topic ' + str(topic[0]+1)+': '+str(topic[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-24T07:16:31.771271Z",
     "start_time": "2018-08-24T07:16:20.465746Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 1: 0.014*\"music\" + 0.014*\"back\" + 0.013*\"return\" + 0.013*\"buy\" + 0.011*\"time\" + 0.011*\"works\" + 0.010*\"sent\" + 0.010*\"work\" + 0.008*\"respond\" + 0.008*\"get\"\n",
      "Topic 2: 0.014*\"device\" + 0.013*\"product\" + 0.012*\"warranty\" + 0.011*\"like\" + 0.010*\"month\" + 0.008*\"support\" + 0.008*\"3\" + 0.008*\"light\" + 0.008*\"customer\" + 0.007*\"work\"\n",
      "Topic 3: 0.021*\"question\" + 0.017*\"music\" + 0.017*\"ask\" + 0.015*\"answer\" + 0.013*\"time\" + 0.013*\"use\" + 0.010*\"work\" + 0.010*\"money\" + 0.010*\"like\" + 0.010*\"know\"\n",
      "Topic 4: 0.015*\"song\" + 0.014*\"music\" + 0.013*\"play\" + 0.010*\"use\" + 0.009*\"speaker\" + 0.007*\"device\" + 0.007*\"thing\" + 0.007*\"understand\" + 0.007*\"never\" + 0.006*\"like\"\n",
      "Topic 5: 0.030*\"work\" + 0.015*\"device\" + 0.013*\"devices\" + 0.012*\"buy\" + 0.011*\"problem\" + 0.010*\"support\" + 0.009*\"another\" + 0.009*\"want\" + 0.008*\"time\" + 0.008*\"money\"\n",
      "Topic 6: 0.029*\"connect\" + 0.018*\"music\" + 0.016*\"work\" + 0.015*\"speaker\" + 0.015*\"play\" + 0.011*\"google\" + 0.011*\"internet\" + 0.011*\"terrible\" + 0.010*\"like\" + 0.010*\"use\"\n",
      "Topic 7: 0.019*\"work\" + 0.016*\"buy\" + 0.011*\"use\" + 0.011*\"device\" + 0.011*\"try\" + 0.010*\"tell\" + 0.009*\"month\" + 0.009*\"time\" + 0.008*\"working\" + 0.008*\"know\"\n",
      "Topic 8: 0.017*\"use\" + 0.017*\"product\" + 0.011*\"need\" + 0.010*\"speak\" + 0.008*\"words\" + 0.008*\"connect\" + 0.007*\"show\" + 0.007*\"disappoint\" + 0.007*\"saying\" + 0.007*\"try\"\n",
      "Topic 9: 0.015*\"music\" + 0.012*\"device\" + 0.010*\"like\" + 0.010*\"disappoint\" + 0.009*\"month\" + 0.009*\"play\" + 0.009*\"want\" + 0.009*\"product\" + 0.009*\"order\" + 0.008*\"purchase\"\n",
      "Topic 10: 0.032*\"sound\" + 0.020*\"speaker\" + 0.016*\"quality\" + 0.014*\"volume\" + 0.010*\"much\" + 0.010*\"loud\" + 0.009*\"use\" + 0.009*\"poor\" + 0.009*\"way\" + 0.009*\"really\"\n"
     ]
    }
   ],
   "source": [
    "\n",
    "topic_test = TopicModelEngine(X_negative)\n",
    "topic_test.stop_words_extention(additional_stop_words)\n",
    "topic_test.LDA_model(10, 'test')\n",
    "topic_test.print_topics(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
