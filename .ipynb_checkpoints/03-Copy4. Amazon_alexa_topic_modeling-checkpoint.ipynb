{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-24T07:43:05.071576Z",
     "start_time": "2018-08-24T07:43:02.985914Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/xzhou/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /Users/xzhou/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import itertools\n",
    "import csv\n",
    "import string\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from collections import Counter\n",
    "import re\n",
    "import spacy\n",
    "import nltk\n",
    "from spacy.lang.en import English\n",
    "from nltk.util import ngrams\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.decomposition import NMF, TruncatedSVD\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import silhouette_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-24T07:43:07.151125Z",
     "start_time": "2018-08-24T07:43:07.143625Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.style.use('seaborn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-24T07:43:07.262421Z",
     "start_time": "2018-08-24T07:43:07.206996Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rating</th>\n",
       "      <th>date</th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>2018-07-31</td>\n",
       "      <td>Sometimes while playing a game, you can answer...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>2018-07-31</td>\n",
       "      <td>I have had a lot of fun with this thing. My 4 ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>2018-07-31</td>\n",
       "      <td>I received the echo as a gift. I needed anothe...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3</td>\n",
       "      <td>2018-07-31</td>\n",
       "      <td>Without having a cellphone, I cannot use many ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>5</td>\n",
       "      <td>2018-07-31</td>\n",
       "      <td>I think this is the 5th one I've purchased. I'...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   rating       date                                             review  \\\n",
       "2       4 2018-07-31  Sometimes while playing a game, you can answer...   \n",
       "3       5 2018-07-31  I have had a lot of fun with this thing. My 4 ...   \n",
       "5       5 2018-07-31  I received the echo as a gift. I needed anothe...   \n",
       "6       3 2018-07-31  Without having a cellphone, I cannot use many ...   \n",
       "7       5 2018-07-31  I think this is the 5th one I've purchased. I'...   \n",
       "\n",
       "   sentiment  \n",
       "2          1  \n",
       "3          1  \n",
       "5          1  \n",
       "6          0  \n",
       "7          1  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pkl_path = '/Users/xzhou/github/project_files/amazon/amazon_alexa_reviews_w_sentiment.pkl'\n",
    "\n",
    "with open(pkl_path, 'rb') as pkl_file:\n",
    "    df = pd.read_pickle(pkl_file)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-24T07:43:07.406276Z",
     "start_time": "2018-08-24T07:43:07.396264Z"
    }
   },
   "outputs": [],
   "source": [
    "# Slice data based on sentiment\n",
    "\n",
    "df_positive = df[df['sentiment']==1]\n",
    "df_neutral = df[df['sentiment']==0]\n",
    "df_negative = df[df['sentiment']==-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-24T07:43:07.683788Z",
     "start_time": "2018-08-24T07:43:07.679814Z"
    }
   },
   "outputs": [],
   "source": [
    "X = df['review']\n",
    "X_positive = df_positive['review']\n",
    "X_neutral = df_neutral['review']\n",
    "X_negative = df_negative['review']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-24T07:43:08.022915Z",
     "start_time": "2018-08-24T07:43:08.006949Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define a topic modeling engine for repetitive use. \n",
    "# This can help test out models and identify optimal result.\n",
    "\n",
    "class TopicModelEngine:\n",
    "    \n",
    "    def __init__(self, raw_data):\n",
    "        self.raw_data = raw_data\n",
    "        self.stop_words = set(stopwords.words('English'))\n",
    "        \n",
    "        \n",
    "    def get_lemma(self, word):\n",
    "        \"\"\"\n",
    "        Get the root words\n",
    "        \"\"\"\n",
    "        try:\n",
    "            lemma = wn.morphy(word)\n",
    "        except:\n",
    "            lemma = word\n",
    "        return lemma\n",
    "    \n",
    "    def stop_words_extention(self, add_stop_words):\n",
    "        \"\"\"\n",
    "        Default stop words are provided by nltk\n",
    "        This is to extend list of stop words. Need to provide a list.  \n",
    "        \"\"\"\n",
    "        add_words_set = set (add_stop_words)\n",
    "        self.stop_words = self.stop_words.union(add_words_set)\n",
    "        \n",
    "        \n",
    "    def tokenize(self, text):\n",
    "        \"\"\"\n",
    "        This is to perform word tokenizing. It performs ngram (1,2)\n",
    "        \"\"\"\n",
    "        lda_tokens = []\n",
    "        text=text.strip().lower()\n",
    "\n",
    "        # Extract word portion out of sentence and excludes spaace\n",
    "        tokenizer = RegexpTokenizer(r'\\w+')\n",
    "        tokens = tokenizer.tokenize(text)\n",
    "        tokens = [token for token in tokens if (not token.isspace())]\n",
    "\n",
    "        for token in tokens:\n",
    "            lda_tokens.append(token)\n",
    "\n",
    "        # Extract bigrams for additional analysis\n",
    "        bigrams = ngrams(tokens, 2)\n",
    "\n",
    "        for bg, count in Counter(bigrams).most_common():\n",
    "            token_string = []\n",
    "            token_string = str(bg[0]) + \" \" + str(bg[1])\n",
    "            lda_tokens.append(token_string)\n",
    "\n",
    "        return lda_tokens\n",
    "    \n",
    "    def text_processing(self, sentence):\n",
    "        \"\"\"\n",
    "        This is to break down text sentence into words, and \n",
    "        clean up the words to prepare for LDA modeling.\n",
    "        \"\"\"\n",
    "    \n",
    "        tokens = self.tokenize(sentence)\n",
    "        tokens = [word for word in tokens if len(tokens)>5]\n",
    "        tokens = [word for word in tokens if word not in self.stop_words]\n",
    "        tokens = [self.get_lemma(word) for word in tokens]\n",
    "\n",
    "        return tokens \n",
    "    \n",
    "    def LDA_preprocessing(self):\n",
    "        \"\"\"\n",
    "        Provided with raw data, the proprocessing will automatically prepare and\n",
    "        clean up the data, and generate necessay documents for LDA's use.\n",
    "        \"\"\"\n",
    "        self.text_data = []\n",
    "\n",
    "        for line in self.raw_data:\n",
    "            tokens = self.text_processing(line)\n",
    "            tokens=[token for token in tokens if token is not None]\n",
    "            self.text_data.append(tokens)\n",
    "        \n",
    "        self.dictionary = corpora.Dictionary(self.text_data)\n",
    "        self.corpus = [self.dictionary.doc2bow(text) for text in self.text_data]\n",
    "\n",
    "    def LDA_model(self, num_of_topics, num_passes, file_name_to_save_as):\n",
    "        \"\"\"\n",
    "        This is to perform topic modeling and save the model under the name\n",
    "        user provides\n",
    "        \"\"\"\n",
    "        self.LDA_preprocessing()\n",
    "        num_topics = num_of_topics\n",
    "        self.lda = LdaModel(corpus=self.corpus,  num_topics=num_topics, \n",
    "                       id2word=self.dictionary, passes=num_passes) \n",
    "        self.lda.save(file_name_to_save_as)\n",
    "        \n",
    "        return self.lda\n",
    "    \n",
    "    def print_topics(self, num_of_words):\n",
    "        \"\"\"\n",
    "        Print out topics for users's decision making. \n",
    "        User can specify number of words to print out\n",
    "        \"\"\"\n",
    "        topics = self.lda.print_topics(num_words=num_of_words)\n",
    "        for topic in topics:\n",
    "            print('Topic ' + str(topic[0]+1)+': '+str(topic[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-24T07:43:08.335742Z",
     "start_time": "2018-08-24T07:43:08.331753Z"
    }
   },
   "outputs": [],
   "source": [
    "# Default stop words are provided by nltk\n",
    "# Additional words to exlude. Those words don't contain meaningful information. \n",
    "# They were identified during multiple iternations\n",
    "\n",
    "additional_words_to_exclude = set(['get', 'one', 'say', 'still', 'amazon',\n",
    "                        'try', 'need', '34', 'echo', 'dot', 'even'\n",
    "                        'like', 'go'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-24T07:43:25.820877Z",
     "start_time": "2018-08-24T07:43:08.708806Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 1: 0.014*\"use\" + 0.013*\"music\" + 0.013*\"know\" + 0.010*\"question\" + 0.010*\"ask\" + 0.009*\"buy\" + 0.009*\"answer\" + 0.009*\"plug\" + 0.009*\"try\" + 0.008*\"able\"\n",
      "Topic 2: 0.019*\"music\" + 0.015*\"play\" + 0.013*\"work\" + 0.010*\"device\" + 0.010*\"time\" + 0.008*\"home\" + 0.008*\"thing\" + 0.008*\"google\" + 0.007*\"much\" + 0.006*\"use\"\n",
      "Topic 3: 0.018*\"work\" + 0.016*\"sound\" + 0.015*\"like\" + 0.012*\"music\" + 0.011*\"really\" + 0.011*\"use\" + 0.011*\"speaker\" + 0.010*\"good\" + 0.010*\"answer\" + 0.010*\"ask\"\n",
      "Topic 4: 0.016*\"product\" + 0.014*\"buy\" + 0.011*\"use\" + 0.009*\"work\" + 0.009*\"music\" + 0.008*\"like\" + 0.008*\"want\" + 0.008*\"phone\" + 0.007*\"month\" + 0.007*\"connect\"\n",
      "Topic 5: 0.016*\"device\" + 0.010*\"buy\" + 0.010*\"connect\" + 0.009*\"work\" + 0.008*\"time\" + 0.008*\"return\" + 0.007*\"speaker\" + 0.007*\"month\" + 0.007*\"working\" + 0.007*\"works\"\n"
     ]
    }
   ],
   "source": [
    "# Identify top 5 negative topics\n",
    "\n",
    "topics_neg5 = TopicModelEngine(X_negative)\n",
    "topics_neg5.stop_words_extention(additional_words_to_exclude)\n",
    "topics_neg5.LDA_model(num_of_topics=5, num_passes=20, file_name_to_save_as='topics_neg5')\n",
    "topics_neg5.print_topics(num_of_words=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-24T07:43:41.113315Z",
     "start_time": "2018-08-24T07:43:26.198334Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 1: 0.021*\"device\" + 0.014*\"work\" + 0.013*\"use\" + 0.010*\"connect\" + 0.009*\"help\" + 0.009*\"customer\" + 0.009*\"warranty\" + 0.009*\"connection\" + 0.007*\"tv\" + 0.006*\"internet\"\n",
      "Topic 2: 0.020*\"music\" + 0.017*\"buy\" + 0.013*\"use\" + 0.012*\"product\" + 0.011*\"device\" + 0.011*\"month\" + 0.011*\"purchase\" + 0.010*\"prime\" + 0.009*\"connect\" + 0.008*\"play\"\n",
      "Topic 3: 0.016*\"device\" + 0.014*\"use\" + 0.013*\"bulb\" + 0.010*\"hub\" + 0.010*\"get\" + 0.009*\"really\" + 0.009*\"hue\" + 0.008*\"find\" + 0.008*\"things\" + 0.008*\"month\"\n",
      "Topic 4: 0.018*\"time\" + 0.017*\"music\" + 0.013*\"work\" + 0.011*\"ask\" + 0.011*\"question\" + 0.010*\"light\" + 0.009*\"times\" + 0.009*\"play\" + 0.009*\"use\" + 0.008*\"like\"\n",
      "Topic 5: 0.025*\"work\" + 0.011*\"connect\" + 0.011*\"try\" + 0.011*\"phone\" + 0.009*\"thing\" + 0.009*\"wifi\" + 0.009*\"instructions\" + 0.009*\"disconnect\" + 0.009*\"buy\" + 0.009*\"product\"\n",
      "Topic 6: 0.013*\"time\" + 0.012*\"month\" + 0.011*\"working\" + 0.010*\"set\" + 0.009*\"dot\" + 0.009*\"product\" + 0.009*\"stop\" + 0.009*\"play\" + 0.008*\"device\" + 0.008*\"return\"\n",
      "Topic 7: 0.035*\"speaker\" + 0.030*\"sound\" + 0.016*\"loud\" + 0.016*\"better\" + 0.013*\"know\" + 0.013*\"use\" + 0.012*\"quality\" + 0.012*\"really\" + 0.011*\"device\" + 0.011*\"volume\"\n",
      "Topic 8: 0.020*\"answer\" + 0.018*\"music\" + 0.018*\"work\" + 0.015*\"question\" + 0.014*\"play\" + 0.014*\"like\" + 0.011*\"ask\" + 0.011*\"know\" + 0.010*\"product\" + 0.009*\"say\"\n",
      "Topic 9: 0.021*\"google\" + 0.014*\"ask\" + 0.011*\"phone\" + 0.011*\"good\" + 0.010*\"buy\" + 0.010*\"like\" + 0.010*\"many\" + 0.009*\"music\" + 0.009*\"even\" + 0.008*\"question\"\n",
      "Topic 10: 0.030*\"work\" + 0.019*\"connect\" + 0.016*\"buy\" + 0.012*\"return\" + 0.011*\"time\" + 0.011*\"like\" + 0.011*\"works\" + 0.010*\"back\" + 0.010*\"make\" + 0.009*\"product\"\n"
     ]
    }
   ],
   "source": [
    "# Identify top 10 negative topics\n",
    "\n",
    "topics_neg10 = TopicModelEngine(X_negative)\n",
    "topics_neg10.stop_words_extention(additional_words_to_exclude)\n",
    "topics_neg10.LDA_model(num_of_topics=10, num_passes=20, file_name_to_save_as='topics_neg10')\n",
    "topics_neg10.print_topics(num_of_words=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-24T07:44:35.331686Z",
     "start_time": "2018-08-24T07:43:41.556873Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 1: 0.051*\"music\" + 0.037*\"love\" + 0.032*\"play\" + 0.020*\"use\" + 0.020*\"weather\" + 0.020*\"ask\" + 0.019*\"fun\" + 0.014*\"question\" + 0.014*\"like\" + 0.013*\"much\"\n",
      "Topic 2: 0.017*\"like\" + 0.016*\"new\" + 0.010*\"love\" + 0.010*\"things\" + 0.009*\"skill\" + 0.008*\"add\" + 0.008*\"time\" + 0.008*\"use\" + 0.008*\"list\" + 0.008*\"voice\"\n",
      "Topic 3: 0.066*\"love\" + 0.029*\"buy\" + 0.019*\"room\" + 0.015*\"house\" + 0.014*\"music\" + 0.014*\"gift\" + 0.013*\"use\" + 0.013*\"speaker\" + 0.013*\"great\" + 0.011*\"another\"\n",
      "Topic 4: 0.090*\"great\" + 0.038*\"works\" + 0.031*\"product\" + 0.030*\"speaker\" + 0.029*\"sound\" + 0.026*\"good\" + 0.021*\"love\" + 0.016*\"better\" + 0.014*\"use\" + 0.014*\"device\"\n",
      "Topic 5: 0.048*\"easy\" + 0.030*\"use\" + 0.030*\"set\" + 0.021*\"home\" + 0.020*\"smart\" + 0.018*\"light\" + 0.014*\"tv\" + 0.013*\"connect\" + 0.013*\"love\" + 0.013*\"control\"\n"
     ]
    }
   ],
   "source": [
    "# Identify top 5 positive topics\n",
    "\n",
    "topics_pos5 = TopicModelEngine(X_positive)\n",
    "topics_pos5.stop_words_extention(additional_words_to_exclude)\n",
    "topics_pos5.LDA_model(num_of_topics=5, num_passes=20, file_name_to_save_as='topics_pos5')\n",
    "topics_pos5.print_topics(num_of_words=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-24T07:45:26.084566Z",
     "start_time": "2018-08-24T07:44:35.668454Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 1: 0.058*\"product\" + 0.054*\"great\" + 0.041*\"works\" + 0.023*\"like\" + 0.021*\"well\" + 0.016*\"new\" + 0.015*\"work\" + 0.015*\"device\" + 0.014*\"expect\" + 0.011*\"recommend\"\n",
      "Topic 2: 0.061*\"speaker\" + 0.056*\"sound\" + 0.031*\"good\" + 0.028*\"better\" + 0.026*\"great\" + 0.025*\"quality\" + 0.018*\"device\" + 0.015*\"price\" + 0.015*\"connect\" + 0.015*\"nice\"\n",
      "Topic 3: 0.065*\"love\" + 0.033*\"buy\" + 0.025*\"use\" + 0.022*\"home\" + 0.019*\"house\" + 0.016*\"dot\" + 0.015*\"day\" + 0.012*\"much\" + 0.012*\"another\" + 0.011*\"2\"\n",
      "Topic 4: 0.155*\"love\" + 0.066*\"great\" + 0.030*\"gift\" + 0.029*\"works\" + 0.022*\"buy\" + 0.019*\"use\" + 0.019*\"good\" + 0.019*\"amaze\" + 0.018*\"learning\" + 0.017*\"things\"\n",
      "Topic 5: 0.021*\"best\" + 0.021*\"buy\" + 0.015*\"know\" + 0.015*\"thing\" + 0.015*\"need\" + 0.015*\"old\" + 0.014*\"year\" + 0.013*\"ever\" + 0.011*\"take\" + 0.011*\"think\"\n",
      "Topic 6: 0.117*\"easy\" + 0.063*\"use\" + 0.062*\"set\" + 0.032*\"love\" + 0.023*\"helpful\" + 0.022*\"setup\" + 0.021*\"great\" + 0.016*\"fire\" + 0.016*\"fun\" + 0.013*\"tv\"\n",
      "Topic 7: 0.025*\"home\" + 0.023*\"smart\" + 0.018*\"voice\" + 0.014*\"control\" + 0.013*\"devices\" + 0.012*\"command\" + 0.010*\"respond\" + 0.009*\"also\" + 0.009*\"google\" + 0.009*\"sometimes\"\n",
      "Topic 8: 0.038*\"music\" + 0.028*\"speaker\" + 0.023*\"room\" + 0.023*\"light\" + 0.021*\"use\" + 0.020*\"alarm\" + 0.017*\"play\" + 0.017*\"great\" + 0.016*\"like\" + 0.015*\"turn\"\n",
      "Topic 9: 0.062*\"fun\" + 0.032*\"like\" + 0.031*\"really\" + 0.029*\"enjoy\" + 0.028*\"much\" + 0.023*\"useful\" + 0.022*\"use\" + 0.019*\"lot\" + 0.016*\"love\" + 0.015*\"great\"\n",
      "Topic 10: 0.071*\"music\" + 0.039*\"play\" + 0.038*\"love\" + 0.027*\"weather\" + 0.024*\"ask\" + 0.019*\"news\" + 0.019*\"tell\" + 0.018*\"question\" + 0.015*\"song\" + 0.013*\"answer\"\n"
     ]
    }
   ],
   "source": [
    "# Identify top 10 positive topics\n",
    "\n",
    "topics_pos10 = TopicModelEngine(X_positive)\n",
    "topics_pos10.stop_words_extention(additional_words_to_exclude)\n",
    "topics_pos10.LDA_model(num_of_topics=10, num_passes=20, file_name_to_save_as='topics_pos10')\n",
    "topics_pos10.print_topics(num_of_words=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
